BigData
------------------
Due to the new technologies, devices, and communication means like social networking sites, the amount of data produced by mankind is growing rapidly every year. 

The amount of data produced by us from the beginning of time till 2018 was 15 billion gigabytes. 

If you pile up the data in the form of disks it may fill an entire football field. The same amount was created in every two days in 2016, and in every ten minutes in 2018. This rate is still growing enormously. 

Though all this information produced is meaningful and can be useful when processed, it is being neglected.

Big data is a collection of large datasets that cannot be processed using traditional computing techniques. It is not a single technique or a tool, rather it has become a complete subject, which involves various tools, techniques and frameworks.

Big data involves the data produced by different devices and applications. 
•Black Box Data - It is a component of helicopter, airplanes, and jets, etc. It captures voices of the flight crew, recordings of microphones and earphones, and the performance information of the aircraft.

•Social Media Data - Social media such as Facebook and Twitter hold information and the views posted by millions of people across the globe.

•Stock Exchange Data - The stock exchange data holds information about the ‘buy’ and ‘sell’ decisions made on a share of different companies made by the customers.

•Power Grid Data - The power grid data holds information consumed by a particular node with respect to a base station.

•Transport Data - Transport data includes model, capacity, distance and availability of a vehicle.

•Search Engine Data - Search engines retrieve lots of data from different databases.

Bigdata :- it is data which is beyond the storage of single centralized server.
Hadoop is a open source java framework for storing large data.
Clusters are set of machine conected to a dedicated network.
Hadoop = storage + Processing
Hadoop is used to store data using HDFS
Hadoop is used to process data using MapReduce which are programs in java.
Hadoop is developed in java.

Benefits of Big Data
•Using the information kept in the social network like Facebook, the marketing agencies are learning about the response for their campaigns, promotions, and other advertising mediums.

•Using the information in the social media for product companies and retail organizations are planning their production.

•Using the data regarding the previous medical history of patients, hospitals are providing better and quick service.

Hadoop is an Apache open source framework written in java that allows distributed processing of large datasets across clusters of computers using simple programming models. 

The Hadoop framework application works in an environment that provides distributed storage and computation across clusters of computers. 
--------------------------------------------------------------------------------
HDFS(Hadoop distributed file system)

Hadoop file system is divided into smaller unit called chunks or blocks.
In Hadoop 1.x the size of block is 64MB.
In Hadoop 2.x the size of block is 128MB.
Hadoop is used to store huge volume of data.
example:-
1)100mb of data will be stored in 2 blocks.(64mb and 36 mb)
2)200mb of data will be stored in 4 blocks.(64mb ,64 mb,64 mb and 8 mb)

In hadoop we use commodity machines which are less expensive machine,like our laptops and desktop to store data.It may fail sometimes.

We have master node or name node as high configuration machine.
name node maintains replica of 3 copies.

name node get heartbeat signals from datanode every 3 seconds.
name node contains metadata about datanode.

master node or name node (server) and datanode is client machines where data will have 3 replica.

for every 10th heartbeat signal or 30 sec block report is send.

block report contains :-
1)number of blocks used.
2)disk usage
3)current activities.
These report are send from datanode to name node.
name node store meta data of entire cluster of datanode.

-------------------------------------------------------------------------------------------------
data ---collection of meaning full data.
they are 3 types
1)structured data:- example oracle,mysql,sql server(rows,columns)
2)Unstructured data:- images ,audio files,video files
3)semi-structured:- files,pdf,xml,json
-----------------------------------------------------------
HDFS
-------------------
1)Job tracker is associated with the name node(master node)
2)Task tracker is associated with the data node.

HDFS cluster is the name given to the whole group systems used for this operation where data is stored.
Mapreduce is the programing model which is used to process the data.

HDFS
--------------------------------------
1)Highly fault-tolerant.(replicate data)
2)high throughput(if 10 machine is used to process the data of 1TB.)the data processing will be faster.
3)suitable for application with large data set.
4)stream access of large set of data.
5)data is build on commodity machines.

HDFS is a file system designed for storing very large files with streaming data access patterns, running clusters on commodity hardware.

*name node is a single point of failure.
---------------------------------------------------------------------
Map reduce
-----------------
A core processing component of hadoop which is meant for processing of huge data in a parallel fashion on commodity hardware machine.

The mapreduce contains two important tasks,namely map and reduce.
Map takes a set of data and converts it into another set of data where individual elements are broken down into tuples(key/value pairs.

secondly reduce task which takes the output from a map as an input and combines those data tuples into a smaller set of tuples. The reduce task is always performed after the map job.

HDFS demons :- name node,secondary name node ,data node
Mapreduce Demons:- Job Tracker ,Task Tracker

 Job Tracker
----------------
1)only one instance of this process runs on a master node same as name node.
2)Any MapReduce job is submitted to job tracker first.
3)Job Tracker checks for the location and initiates tasks to Data nodes by communicating with task tracker Daemons.

Task Tracker
--------------------------
1)This process has multiple instances running on the slave nodes where data node is running.
2)it receives the job instractions from job tracker daemon.
3)the task tracker initiates the task on the same node where physical data block is present.
4)same as the data node daemon this process also periodically sends heart bits to job tracker to make jobtracker aware that slave process is running.

Data Flow In MapReduce
-------------------------------------------
1)Input reader
The input reader reads the upcoming data and splits it into the data blocks of the appropriate size (64 MB to 128 MB). Each data block is associated with a Map function.

Once input reads the data, it generates the corresponding key-value pairs. The input files reside in HDFS

2)Map function
The map function process the upcoming key-value pairs and generated the corresponding output key-value pairs. The map input and output type may be different from each other.

3)Partition function
The partition function assigns the output of each Map function to the appropriate reducer. The available key and value provide this function. It returns the index of reducers.

4)Shuffling and Sorting
The data are shuffled between/within nodes so that it moves out from the map and get ready to process for reduce function. Sometimes, the shuffling of data can take much computation time.

5)The sorting operation is performed on input data for Reduce function. Here, the data is compared using comparison function and arranged in a sorted form.

6)Reduce function
The Reduce function is assigned to each unique key. These keys are already arranged in sorted order. The values associated with the keys can iterate the Reduce and generates the corresponding output.

7)Output writer
Once the data flow from all the above phases, Output writer executes. The role of Output writer is to write the Reduce output to the stable storage.

------------------------------------------------------------------------------------------------

 




